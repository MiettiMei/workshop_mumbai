{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning with `nilearn`\n",
    "\n",
    "Although nilearn's visualizations are quite nice, its primary purpose was to facilitate machine learning in neuroimaging. It's in some sense the bridge between [nibabel](http://nipy.org/nibabel/) and [scikit-learn](http://scikit-learn.org/stable/). On the one hand, it reformats images to be easily passed to scikit-learn, and on the other, it reformats the results to produce valid nibabel images.\n",
    "\n",
    "So let's take a look at a short multi-variate pattern analysis (MVPA) example.\n",
    "\n",
    "**Note**: This section is heavily based on the [nilearn decoding tutorial](https://nilearn.github.io/auto_examples/plot_decoding_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import nibabel as nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load machine learning dataset\n",
    "\n",
    "Let's load the dataset we prepared in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = '/home/neuro/notebooks/data/dataset_ML.nii.gz'\n",
    "!nib-ls $func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mask\n",
    "\n",
    "As we only want to use voxels in a particular region of interest (ROI) for the classification, let's create a function that returns a mask that either contains the only the brain, only the eyes or both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import resample_to_img, math_img\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "def get_mask(mask_type):\n",
    "    \n",
    "    # Specify location of the brain and eye image\n",
    "    brain = '/templates/MNI152_T1_1mm_brain.nii.gz'\n",
    "    eyes = '/templates/MNI152_T1_1mm_eye.nii.gz'\n",
    "\n",
    "    # Load region of interest\n",
    "    if mask_type == 'brain':\n",
    "        img_resampled = resample_to_img(brain, func)\n",
    "    elif mask_type == 'eyes':\n",
    "        img_resampled = resample_to_img(eyes, func)\n",
    "    elif mask_type == 'both':\n",
    "        img_roi = math_img(\"img1 + img2\", img1=brain, img2=eyes)\n",
    "        img_resampled = resample_to_img(img_roi, func)\n",
    "\n",
    "    # Binarize ROI template\n",
    "    data_binary = np.array(img_resampled.get_fdata()>=10, dtype=np.int8)\n",
    "\n",
    "    # Dilate binary mask once\n",
    "    data_dilated = binary_dilation(data_binary, iterations=1).astype(np.int8)\n",
    "\n",
    "    # Save binary mask in NIfTI image\n",
    "    mask = nb.Nifti1Image(data_dilated, img_resampled.affine, img_resampled.header)\n",
    "    mask.set_data_dtype('i1')\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking and Un-masking data\n",
    "\n",
    "For the classification with `nilearn` we need our functional data in a 2D, sample-by-voxel matrix. To get that, we'll select all the voxels defined in our `mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_roi\n",
    "anat = '/templates/MNI152_T1_1mm.nii.gz'\n",
    "mask = get_mask('both')\n",
    "plot_roi(mask, anat, cmap='Paired', dim=-.5, draw_cross=False, annotate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NiftiMasker` is an object that applies a mask to a dataset and returns the masked voxels as a vector at each time point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "masker = NiftiMasker(mask_img=mask, standardize=False, detrend=False)\n",
    "samples = masker.fit_transform(func)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its shape corresponds to the number of time-points times the number of voxels in the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recover the original data shape (giving us a masked and z-scored BOLD series), we simply use the masker's inverse transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_epi = masker.inverse_transform(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the masked epi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import math_img\n",
    "from nilearn.plotting import plot_stat_map\n",
    "\n",
    "max_zscores = math_img(\"np.abs(img).max(axis=3)\", img=masked_epi)\n",
    "plot_stat_map(max_zscores, bg_img=anat, dim=-.5, cut_coords=[33, -20, 20],\n",
    "              draw_cross=False, annotate=False, colorbar=False,\n",
    "              title='Maximum Amplitude per Voxel in Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MVPA Example\n",
    "\n",
    "Multi-voxel pattern analysis (MVPA) is a general term for techniques that contrast conditions over multiple voxels. It's very common to use machine learning models to generate statistics of interest.\n",
    "\n",
    "In this case, we'll use the response patterns of voxels in the mask to predict if the eyes were **closed** or **open** during a resting-state fMRI recording. We'll use a support vector classifier (SVC) and leave-one-run-out cross-validation.\n",
    "\n",
    "**Note:** This section is not intended to teach machine learning, but to demonstrate a simple nilearn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels file contains metadata for each volume, indicating the stimulus type and subject number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = '/home/neuro/notebooks/data/labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 17 $labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `np.recfromcsv()`, we can refer to each column of this file by its header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = np.recfromcsv(labels, delimiter=\" \")\n",
    "attrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli, runs = attrs['labels'], attrs['chunks']\n",
    "print(np.unique(stimuli))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave-one-subject-out cross-validation trains on `(n - 1)` subjects, and classifies the remaining subject, for each subject. Mean (across subject) cross-validation accuracy is a common statistic for classification-based MVPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's specify the classifier\n",
    "clf = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Performe the cross validation (takes time to compute)\n",
    "cva = cross_val_score(estimator=clf,\n",
    "                      X=samples,\n",
    "                      y=stimuli,\n",
    "                      groups=runs,\n",
    "                      cv=LeaveOneGroupOut(),\n",
    "                      n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the cross validation was computed we can extract the overall accuracy, as well as the accuracy for each individual fold (i.e. leave-one-subject-out prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average accuracy = %.02f percent' % (cva.mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy per fold:', cva, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wow, 86.46% accuracy!!!** That's great! But with a simple MVPA approach we unfortunately don't know which regions are driving the classification accuracy. We just know that all voxels in the mask allow the classification of the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same same, but different\n",
    "\n",
    "Let's do the same MVPA approach again, but this time with a `LogisticRegression` classifier and a mask that only keeps the voxels around the eyes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = NiftiMasker(mask_img=get_mask('eyes'), standardize=False, detrend=False)\n",
    "samples = masker.fit_transform(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cva = cross_val_score(estimator=clf,\n",
    "                      X=samples,\n",
    "                      y=stimuli,\n",
    "                      groups=runs,\n",
    "                      cv=LeaveOneGroupOut(),\n",
    "                      n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average accuracy = %.02f percent' % (cva.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.. 80.47% is still great, but worse than before. We need a better technique that tells us where in head we should look. Luckily, there exists the **Searchlight** approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searchlight approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The radius is the one of the Searchlight sphere that will scan the volume\n",
    "searchlight = decoding.SearchLight(\n",
    "    get_mask('eyes'),\n",
    "    process_mask_img=get_mask('eyes'),\n",
    "    radius=5.6, n_jobs=-1,\n",
    "    verbose=1, cv=LeaveOneGroupOut())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "searchlight.fit(nb.load(func), stimuli, groups=runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fmri mean image as a surrogate of anatomical data\n",
    "from nilearn import image\n",
    "mean_fmri = image.mean_img(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_stat_map, plot_img, show\n",
    "searchlight_img = new_img_like(mean_fmri, searchlight.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_glass_brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glass_brain(searchlight_img, threshold=0.6, cmap='bwr', black_bg=True, colorbar=True, display_mode='lyrz', vmax=0.7)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
