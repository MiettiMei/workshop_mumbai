{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyMVPA\n",
    "\n",
    "[PyMVPA](http://www.pymvpa.org/) is a Python package intended to ease statistical learning analyses of large datasets. It offers an extensible framework with a high-level interface to a broad range of algorithms for classification, regression, feature selection, data import and export.\n",
    "\n",
    "The power in PyMVPA lies in it's flexibility with classifier. PyMVPA is able to use many classifiers from LIBSVM and scikit-learn, and the overall list that are at your hands is impressive. The following are only some of the classifiers that you can chose from:\n",
    "\n",
    "- Bayesian Linear Regression (BLR)\n",
    "- Elastic-Net (ENET) regression classifier\n",
    "- Gaussian Discriminant Analyses (LDA and QDA)\n",
    "- Gaussian Naive Bayes Classifier (GNB)\n",
    "- Gaussian Process Regression (GPR)\n",
    "- GLM-Net (GLMNET) regression and classifier\n",
    "- k-Nearest-Neighbour classifier (kNN)\n",
    "- Least angle regression (LARS)\n",
    "- Penalized logistic regression classifier\n",
    "- Similarity functions for prototype-based projection\n",
    "- Sparse Multinomial Logistic Regression classifier (SMLR)\n",
    "- SVM and SVR machines\n",
    "\n",
    "**Note:** The content of this notebook is taken and adapted from the [PyMVPA](http://www.pymvpa.org/) homepage and serves an illustrative purpose. For more information and better understanding, go directly to [PyMVPA](http://www.pymvpa.org/).\n",
    "\n",
    "Having said so, let's take a look at PyMVPA's **Searchlight** example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Dataset\n",
    "\n",
    "To do anything in this tutoroial, we first need to download some tutorial data. This can be done with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl http://data.pymvpa.org/datasets/tutorial_data/tutorial_data-0.4.tar.gz | tar xvz \\\n",
    "    -C /home/neuro/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv /home/neuro/tutorial_data/data /home/neuro/tutorial_data/haxby2001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searchlight on fMRI data\n",
    "\n",
    "The original idea of a spatial searchlight algorithm stems from a paper by [Kriegeskorte et al. (2006)](http://www.pymvpa.org/references.html#kgb06), and has subsequently been used in a number of studies. The most common use for a searchlight is to compute a full cross-validation analysis in each spherical region of interest (ROI) in the brain. This analysis yields a map of (typically) classification accuracies that are often interpreted or post-processed similar to a GLM statistics output map (e.g. subsequent analysis with inferential statistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant modules\n",
    "from mvpa2.suite import *\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As searchlight analyses are usually quite expensive in terms of computational resources, we are going to enable some progress output to entertain us while we are waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable debug output for searchlight call\n",
    "if __debug__:\n",
    "    debug.active += [\"SLC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvpa2.tutorial_suite import fmri_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = '/home/neuro/notebooks/data/dataset_ML.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import resample_to_img, math_img\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "def get_mask(mask_type):\n",
    "    \n",
    "    # Specify location of the brain and eye image\n",
    "    brain = '/templates/MNI152_T1_1mm_brain.nii.gz'\n",
    "    eyes = '/templates/MNI152_T1_1mm_eye.nii.gz'\n",
    "\n",
    "    # Load region of interest\n",
    "    if mask_type == 'brain':\n",
    "        img_resampled = resample_to_img(brain, func)\n",
    "    elif mask_type == 'eyes':\n",
    "        img_resampled = resample_to_img(eyes, func)\n",
    "    elif mask_type == 'both':\n",
    "        img_roi = math_img(\"img1 + img2\", img1=brain, img2=eyes)\n",
    "        img_resampled = resample_to_img(img_roi, func)\n",
    "\n",
    "    # Binarize ROI template\n",
    "    data_binary = np.array(img_resampled.get_fdata()>=10, dtype=np.int8)\n",
    "\n",
    "    # Dilate binary mask once\n",
    "    data_dilated = binary_dilation(data_binary, iterations=1).astype(np.int8)\n",
    "\n",
    "    # Save binary mask in NIfTI image\n",
    "    mask = nb.Nifti1Image(data_dilated, img_resampled.affine, img_resampled.header)\n",
    "    mask.set_data_dtype('i1')\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = get_mask('both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.to_filename('bla.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = '/home/neuro/notebooks/data/labels.txt'\n",
    "attrs = np.recfromcsv(labels, delimiter=\" \")\n",
    "stimuli, runs = attrs['labels'], attrs['chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = fmri_dataset(samples=func,\n",
    "                  targets=[str(s)[2:-1] for s in stimuli],\n",
    "                  chunks=runs,\n",
    "                  mask='bla.nii.gz')\n",
    "\n",
    "del ds.sa['time_coords']\n",
    "del ds.sa['time_indices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.copy(deep=False,\n",
    "             sa=['targets', 'chunks'],\n",
    "             fa=['voxel_indices'],\n",
    "             a=['mapper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join as opj\n",
    "from mvpa2.base.hdf5 import h5load, h5save\n",
    "from mvpa2.clfs.svm import LinearCSVMC, LinearNuSVMC\n",
    "from mvpa2.clfs.smlr import SMLR\n",
    "from mvpa2.generators.partition import NFoldPartitioner\n",
    "from mvpa2.measures.base import CrossValidation\n",
    "from mvpa2.measures.searchlight import sphere_searchlight\n",
    "from mvpa2.misc.errorfx import mean_match_accuracy\n",
    "from mvpa2.mappers.fx import mean_sample\n",
    "from mvpa2.suite import map2nifti, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_scattered_results(sl, dataset, roi_ids, results):\n",
    "    \"\"\"Function to aggregate results - This requires the searchlight\n",
    "    conditional attribute 'roi_feature_ids' to be enabled\"\"\"\n",
    "    import numpy as np\n",
    "    from mvpa2.datasets import Dataset\n",
    "    resmap = None\n",
    "    for resblock in results:\n",
    "        for res in resblock:\n",
    "            if resmap is None:\n",
    "                # prepare the result container\n",
    "                resmap = np.zeros((len(res), dataset.nfeatures),\n",
    "                                  dtype=res.samples.dtype)\n",
    "                observ_counter = np.zeros(dataset.nfeatures, dtype=int)\n",
    "            # project the result onto all features -- love broadcasting!\n",
    "            resmap[:, res.a.roi_feature_ids] += res.samples\n",
    "            # increment observation counter for all relevant features\n",
    "            observ_counter[res.a.roi_feature_ids] += 1\n",
    "    # when all results have been added up average them according to the number\n",
    "    # of observations\n",
    "    observ_mask = observ_counter > 0\n",
    "    resmap[:, observ_mask] /= observ_counter[observ_mask]\n",
    "    result_ds = Dataset(resmap,\n",
    "                        fa={'observations': observ_counter})\n",
    "    if 'mapper' in dataset.a:\n",
    "        import copy\n",
    "        result_ds.a['mapper'] = copy.copy(dataset.a.mapper)\n",
    "    return result_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify balancer and partitioner\n",
    "partitioner = NFoldPartitioner(cvtype=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfmode = 'LinearCSVMC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose right classifier\n",
    "if clfmode == 'LinearCSVMC':\n",
    "    clf = LinearCSVMC()\n",
    "\n",
    "elif clfmode == 'LinearNuSVMC':\n",
    "    clf = LinearNuSVMC()\n",
    "\n",
    "elif clfmode == 'SMLR':\n",
    "    smlr_lm = 0.1\n",
    "    clf = SMLR(lm=smlr_lm)  # [1e-10,inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidation(clf, partitioner,\n",
    "                     errorfx=mean_match_accuracy,\n",
    "                     enable_ca=['stats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphere_radius = 2\n",
    "nth_element = 100\n",
    "cores2use = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = sphere_searchlight(cv,\n",
    "                        radius=sphere_radius,\n",
    "                        center_ids=range(0,\n",
    "                                         ds.shape[1],\n",
    "                                         nth_element),\n",
    "                        space='voxel_indices',\n",
    "                        results_fx=fill_in_scattered_results,\n",
    "                        postproc=mean_sample(),\n",
    "                        enable_ca=['calling_time', 'roi_feature_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl.nproc = cores2use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train classifier on original and permutated dataset\n",
    "sl_map = sl(ds)\n",
    "print('orig done after %s' % (\n",
    "    time.strftime(\n",
    "        '%H:%M:%S',\n",
    "        time.gmtime(round(sl.ca.calling_time)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Classifier Specific outputs\n",
    "accuracies = sl_map.samples[0]\n",
    "mean_accuracy = accuracies.mean()\n",
    "std_accuracy = accuracies.std()\n",
    "chance_level = 1.0 / len(ds.sa.get(space).unique)\n",
    "\n",
    "threshold_above_average = lambda x: chance_level + x * std_accuracy\n",
    "spheres_above_average = lambda x: np.sum(\n",
    "    accuracies >= threshold_above_average(x))\n",
    "percent_above_average = lambda x: np.mean(\n",
    "    accuracies >= threshold_above_average(x)) * 100\n",
    "\n",
    "# Save searchlight accuracy map to NIfTI file\n",
    "niftiresults = map2nifti(ds, data=sl_map.S, imghdr=ds.a.imghdr)\n",
    "niftiresults.to_filename(opj(outpath, 'nifti_%s.nii.gz' % identifier))\n",
    "\n",
    "# Write report to file\n",
    "csvfile = opj(outpath, 'report_%s.rst' % identifier)\n",
    "with open(csvfile, 'w') as f:\n",
    "    f.write('Subject          : {0}\\n'.format(sub))\n",
    "    f.write('Classifier       : {0}\\n'.format(clfmode))\n",
    "    f.write('Category         : {0}\\n'.format(category))\n",
    "    f.write('Classes          : {0}\\n'.format(classes))\n",
    "    f.write('Comparison       : {0}\\n'.format(comparison))\n",
    "    f.write('Sphere Radius    : {0}\\n'.format(sphere_radius))\n",
    "    f.write('N-th Element     : {0}\\n'.format(nth_element))\n",
    "    f.write('Wall Time        : {0}\\n'.format(\n",
    "        time.strftime('%H:%M:%S', time.gmtime(round(sl.ca.calling_time)))))\n",
    "    f.write('Samples          : {0}\\n'.format(ds.S.shape[0]))\n",
    "    f.write('Features         : {0}\\n'.format(ds.S.shape[1]))\n",
    "    f.write('Volume Dimension : {0}\\n'.format(str(ds.a.voxel_dim)))\n",
    "    f.write('Voxel  Dimension : {0}\\n'.format(str(ds.a.voxel_eldim)))\n",
    "    f.write('endfix           : {0}\\n'.format(endfix))\n",
    "    f.write('CPU              : {0}\\n'.format(cores2use))\n",
    "\n",
    "    f.write('\\nChance Level     : {0}\\n'.format(round(chance_level, 5)))\n",
    "    f.write('Accuracy (mean)  : {0:5}\\n'.format(round(mean_accuracy, 5)))\n",
    "    f.write('Accuracy (std)   : {0:5}\\n\\n'.format(round(std_accuracy, 5)))\n",
    "\n",
    "    f.write('%Sphere > +2STD  : {0:5}%\\n'.format(round(percent_above_average(2), 3)))\n",
    "    f.write('%Sphere > +3STD  : {0:5}%\\n'.format(round(percent_above_average(3), 3)))\n",
    "    f.write('%Sphere > +4STD  : {0:5}%\\n'.format(round(percent_above_average(4), 3)))\n",
    "    f.write('vSphere > +2STD  : {0:5}\\n'.format(spheres_above_average(2)))\n",
    "    f.write('vSphere > +3STD  : {0:5}\\n'.format(spheres_above_average(3)))\n",
    "    f.write('vSphere > +4STD  : {0:5}\\n'.format(spheres_above_average(4)))\n",
    "\n",
    "    f.write('\\n\\nDataset Summary:\\n')\n",
    "    f.write('****************\\n')\n",
    "    f.write('%s' % ds.summary())\n",
    "    f.write('%s' % ds.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sa.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sa.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now for the interesting part: Next we define the measure that shall be computed for each sphere. Theoretically, this can be anything, but here we choose to compute a full leave-one-out cross-validation using a linear Nu-SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose classifier\n",
    "clf = LinearNuSVMC()\n",
    "\n",
    "# setup measure to be computed by Searchlight\n",
    "# cross-validated mean transfer using an N-fold dataset splitter\n",
    "cv = CrossValidation(clf, NFoldPartitioner())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we do not want to compute full-brain accuracy maps, but instead limit ourselves to a specific subset of voxels. We’ll select all voxel that have a non-zero z-stats value in the localizer mask we loaded above, as center coordinates for a searchlight sphere. These spheres will still include voxels that did not pass the threshold. the localizer merely define the location of all to be processed spheres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ids of features that have a nonzero value\n",
    "center_ids = ds.fa.voxel_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run the searchlight. We’ll perform the analysis for three different radii, each time computing an error for each sphere. To achieve this, we simply use the [sphere_searchlight()](http://www.pymvpa.org/generated/mvpa2.measures.searchlight.sphere_searchlight.html#mvpa2.measures.searchlight.sphere_searchlight) class, which takes any [processing object](http://www.pymvpa.org/glossary.html#term-processing-object) and a radius as arguments. The [processing object](http://www.pymvpa.org/glossary.html#term-processing-object) has to compute the intended measure, when called with a dataset. The [sphere_searchlight()](http://www.pymvpa.org/generated/mvpa2.measures.searchlight.sphere_searchlight.html#mvpa2.measures.searchlight.sphere_searchlight) object will do nothing more than generate small datasets for each sphere, feeding them to the processing object, and storing the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup plotting parameters (not essential for the analysis itself)\n",
    "plot_args = {\n",
    "    'background' : os.path.join(datapath, 'haxby2001', 'sub001', 'anatomy', 'highres001.nii.gz'),\n",
    "    'background_mask' : os.path.join(datapath, 'haxby2001', 'sub001', 'masks', 'orig', 'brain.nii.gz'),\n",
    "    'overlay_mask' : os.path.join(datapath, 'haxby2001', 'sub001', 'masks', 'orig', 'vt.nii.gz'),\n",
    "    'do_stretch_colors' : False,\n",
    "    'cmap_bg' : 'gray',\n",
    "    'cmap_overlay' : 'autumn', # YlOrRd_r # pl.cm.autumn\n",
    "    'interactive' : cfg.getboolean('examples', 'interactive', True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Searchlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose Sphere radius\n",
    "radius = 0\n",
    "\n",
    "# tell which one we are doing\n",
    "print(\"Running searchlight with radius: %i ...\" % (radius))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we actually setup the spherical searchlight by configuring the radius, and our selection of sphere center coordinates. Moreover, via the **space** argument we can instruct the searchlight which feature attribute shall be used to determine the voxel neighborhood. By default, fmri_dataset() creates a corresponding attribute called **voxel_indices**. Using the **mapper** argument it is possible to post-process the results computed for each sphere. Cross-validation will compute an error value per each fold, but here we are only interested in the mean error across all folds. Finally, on multi-core machines **nproc** can be used to enabled parallelization by setting it to the number of processes utilized by the searchlight (default value of **nproc`=`None** utilizes all available local cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = sphere_searchlight(cv, radius=radius, space='voxel_indices',\n",
    "                        center_ids=center_ids,\n",
    "                        postproc=mean_sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we care about efficiency, we are stripping all attributes from the dataset that are not required for the searchlight analysis. This will offers some speedup, since it reduces the time that is spent on dataset slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset.copy(deep=False,\n",
    "                  sa=['targets', 'chunks'],\n",
    "                  fa=['voxel_indices'],\n",
    "                  a=['mapper'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we actually run the analysis. The result is returned as a dataset. For the upcoming plots, we are transforming the returned error maps into accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the serachlight\n",
    "sl_map = sl(ds)\n",
    "\n",
    "# Changes output from error maps to accuracy maps\n",
    "sl_map.samples *= -1\n",
    "sl_map.samples += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigat the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result dataset is fully aware of the original dataspace. Using this information we can map the 1D accuracy maps back into “brain-space” (using NIfTI image header information from the original input timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niftiresults = map2nifti(sl_map, imghdr=dataset.a.imghdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMVPA comes with a convenient plotting function to visualize the searchlight maps. We are only looking at fMRI slices that are covered by the mask of ventral temproal cortex.\n",
    "\n",
    "The following figures show the resulting accuracy maps for the slices covered by the ventral temporal cortex mask. Note that each voxel value represents the accuracy of a sphere centered around this voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Searchlight (single element; univariate) accuracy maps '\n",
    "      'for binary classification house vs. scrambledpix.')\n",
    "fig = pl.figure(figsize=(12, 8), facecolor='white')\n",
    "subfig = plot_lightbox(overlay=niftiresults,\n",
    "                       vlim=(0.0, None), slices=range(23,29),\n",
    "                       fig=fig, **plot_args);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With radius 0 (only the center voxel is part of the part the sphere) there is a clear distinction between two distributions. The chance distribution, relatively symetric and centered around the expected chance-performance at 50%. The second distribution, presumambly of voxels with univariate signal, is nicely segregated from that. \n",
    "\n",
    "### Run searchlight again, but this time with a sphere radius of 3\n",
    "\n",
    "Increasing the searchlight size significantly blurrs the accuracy map, but also lead to an increase in classification accuracy. So let's try the searchlight again with a sphere radius of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 3\n",
    "sl = sphere_searchlight(cv, radius=radius, space='voxel_indices',\n",
    "                        center_ids=center_ids, postproc=mean_sample())\n",
    "sl_map = sl(ds)\n",
    "sl_map.samples *= -1\n",
    "sl_map.samples += 1\n",
    "niftiresults = map2nifti(sl_map, imghdr=dataset.a.imghdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Searchlight (radius 3 elements; 123 voxels) accuracy maps for '\n",
    "      'binary classification house vs. scrambledpix.')\n",
    "fig = pl.figure(figsize=(12, 8), facecolor='white')\n",
    "subfig = plot_lightbox(\n",
    "    overlay=niftiresults, vlim=(0.0, None),\n",
    "    slices=range(23,29), fig=fig, **plot_args)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
